"""Ollama integration tests.

These tests require a running Ollama instance with a model installed.
Enable by setting environment variables:
    OLLAMA_INTEGRATION_TESTS=1
    OLLAMA_TEST_MODEL=tinyllama  (or any small model you have installed)

To install Ollama and pull a small model:
    curl -fsSL https://ollama.com/install.sh | sh
    ollama pull tinyllama
"""

from __future__ import annotations

import json
import os
from typing import Any

import pytest

OLLAMA_MODEL = os.environ.get("OLLAMA_TEST_MODEL", "tinyllama")
OLLAMA_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")


@pytest.fixture
def ollama_service() -> Any:
    """Create an OllamaService configured for testing."""
    from unittest.mock import patch

    with (
        patch("app.config.settings.ollama_base_url", OLLAMA_URL),
        patch("app.config.settings.ollama_model", OLLAMA_MODEL),
    ):
        from app.services.ai.ollama import OllamaService

        return OllamaService()


@pytest.mark.asyncio
class TestOllamaRecipeGeneration:
    async def test_generate_recipes_returns_list(self, ollama_service: Any) -> None:
        """Test that Ollama can generate recipes and returns a list."""
        result = await ollama_service.generate_recipes(
            prompt="simple pasta recipe",
            available_ingredients=["pasta", "tomatoes", "garlic", "olive oil"],
            dietary_preferences=[],
            health_goals=[],
            family_dietary_notes=[],
            favorite_cuisines=[],
            max_results=1,
            max_prep_time=None,
            cuisine=None,
        )
        assert isinstance(result, list)

    async def test_generate_recipes_with_dietary_preferences(
        self, ollama_service: Any
    ) -> None:
        """Test recipe generation respects dietary preferences."""
        result = await ollama_service.generate_recipes(
            prompt="dinner recipe",
            available_ingredients=["rice", "vegetables", "tofu"],
            dietary_preferences=["vegan"],
            health_goals=[],
            family_dietary_notes=[],
            favorite_cuisines=[],
            max_results=1,
            max_prep_time=None,
            cuisine=None,
        )
        assert isinstance(result, list)

    async def test_generate_recipes_with_cuisine_filter(
        self, ollama_service: Any
    ) -> None:
        """Test recipe generation with a specific cuisine filter."""
        result = await ollama_service.generate_recipes(
            prompt="dinner",
            available_ingredients=["chicken", "rice"],
            dietary_preferences=[],
            health_goals=[],
            family_dietary_notes=[],
            favorite_cuisines=[],
            max_results=1,
            max_prep_time=30,
            cuisine="Italian",
        )
        assert isinstance(result, list)


@pytest.mark.asyncio
class TestOllamaSubstitutions:
    async def test_suggest_substitutions(self, ollama_service: Any) -> None:
        """Test that Ollama can suggest ingredient substitutions."""
        result = await ollama_service.suggest_substitutions(
            original_ingredient="butter",
            dietary_restrictions=["dairy-free"],
            available_ingredients=["coconut oil", "margarine", "olive oil"],
        )
        assert isinstance(result, list)

    async def test_suggest_substitutions_empty_restrictions(
        self, ollama_service: Any
    ) -> None:
        """Test substitution suggestions without dietary restrictions."""
        result = await ollama_service.suggest_substitutions(
            original_ingredient="all-purpose flour",
            dietary_restrictions=[],
            available_ingredients=[],
        )
        assert isinstance(result, list)


@pytest.mark.asyncio
class TestOllamaVoiceParsing:
    async def test_parse_voice_input(self, ollama_service: Any) -> None:
        """Test that Ollama can parse voice input for ingredients."""
        result = await ollama_service.parse_voice_input(
            "two cups of flour and three eggs"
        )
        assert isinstance(result, dict)

    async def test_parse_complex_voice_input(self, ollama_service: Any) -> None:
        """Test parsing more complex voice input."""
        result = await ollama_service.parse_voice_input(
            "I have half a pound of chicken breast, one onion, "
            "and about a quarter cup of soy sauce"
        )
        assert isinstance(result, dict)


@pytest.mark.asyncio
class TestOllamaPromptIntegrity:
    """Verify that the prompts generated by the base class are valid for Ollama."""

    async def test_recipe_prompt_produces_parseable_output(
        self, ollama_service: Any
    ) -> None:
        """Check that Ollama returns JSON-parseable recipe output."""
        result = await ollama_service.generate_recipes(
            prompt="make me a quick soup",
            available_ingredients=["chicken broth", "noodles", "carrots"],
            dietary_preferences=[],
            health_goals=["low sodium"],
            family_dietary_notes=[],
            favorite_cuisines=["American"],
            max_results=1,
            max_prep_time=20,
            cuisine=None,
        )
        # Verify it's valid JSON by the time we get it (already parsed in the service)
        assert isinstance(result, list)
        # If we get recipes, check they have expected keys
        if len(result) > 0:
            recipe = result[0]
            assert isinstance(recipe, dict)
